{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPSGksPCpKT1VNn5b8JpSD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adigolbari/finetune-transformer-lm/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "6ynBKvDtDRfl",
        "outputId": "aa37a0ba-a396-447d-f208-e2466c3d88cf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'opt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c7a6cec031f2>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_cosine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_linear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrocstories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0manalysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrocstories\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrocstories_analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'opt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "import joblib\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from opt import adam, warmup_cosine, warmup_linear, warmup_constant\n",
        "from datasets import rocstories\n",
        "from analysis import rocstories as rocstories_analysis\n",
        "from text_utils import TextEncoder\n",
        "from utils import encode_dataset, flatten, iter_data, find_trainable_variables, convert_gradient_to_tensor, shape_list, ResultLogger, assign_to_gpu, average_grads, make_path\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5*x*(1+tf.tanh(math.sqrt(2/math.pi)*(x+0.044715*tf.pow(x, 3))))\n",
        "\n",
        "def swish(x):\n",
        "    return x*tf.nn.sigmoid(x)\n",
        "\n",
        "opt_fns = {\n",
        "    'adam':adam,\n",
        "}\n",
        "\n",
        "act_fns = {\n",
        "    'relu':tf.nn.relu,\n",
        "    'swish':swish,\n",
        "    'gelu':gelu\n",
        "}\n",
        "\n",
        "lr_schedules = {\n",
        "    'warmup_cosine':warmup_cosine,\n",
        "    'warmup_linear':warmup_linear,\n",
        "    'warmup_constant':warmup_constant,\n",
        "}\n",
        "\n",
        "def _norm(x, g=None, b=None, e=1e-5, axis=[1]):\n",
        "    u = tf.reduce_mean(x, axis=axis, keep_dims=True)\n",
        "    s = tf.reduce_mean(tf.square(x-u), axis=axis, keep_dims=True)\n",
        "    x = (x - u) * tf.rsqrt(s + e)\n",
        "    if g is not None and b is not None:\n",
        "        x = x*g + b\n",
        "    return x\n",
        "\n",
        "def norm(x, scope, axis=[-1]):\n",
        "    with tf.variable_scope(scope):\n",
        "        n_state = shape_list(x)[-1]\n",
        "        g = tf.get_variable(\"g\", [n_state], initializer=tf.constant_initializer(1))\n",
        "        b = tf.get_variable(\"b\", [n_state], initializer=tf.constant_initializer(0))\n",
        "        return _norm(x, g, b, axis=axis)\n",
        "\n",
        "def dropout(x, pdrop, train):\n",
        "    if train and pdrop > 0:\n",
        "        x = tf.nn.dropout(x, 1-pdrop)\n",
        "    return x\n",
        "\n",
        "def mask_attn_weights(w):\n",
        "    n = shape_list(w)[-1]\n",
        "    b = tf.matrix_band_part(tf.ones([n, n]), -1, 0)\n",
        "    b = tf.reshape(b, [1, 1, n, n])\n",
        "    w = w*b + -1e9*(1-b)\n",
        "    return w\n",
        "\n",
        "def _attn(q, k, v, train=False, scale=False):\n",
        "    w = tf.matmul(q, k)\n",
        "\n",
        "    if scale:\n",
        "        n_state = shape_list(v)[-1]\n",
        "        w = w*tf.rsqrt(tf.cast(n_state, tf.float32))\n",
        "\n",
        "    w = mask_attn_weights(w)\n",
        "    w = tf.nn.softmax(w)\n",
        "\n",
        "    w = dropout(w, attn_pdrop, train)\n",
        "\n",
        "    a = tf.matmul(w, v)\n",
        "    return a\n",
        "\n",
        "def split_states(x, n):\n",
        "    x_shape = shape_list(x)\n",
        "    m = x_shape[-1]\n",
        "    new_x_shape = x_shape[:-1]+[n, m//n]\n",
        "    return tf.reshape(x, new_x_shape)\n",
        "\n",
        "def merge_states(x):\n",
        "    x_shape = shape_list(x)\n",
        "    new_x_shape = x_shape[:-2]+[np.prod(x_shape[-2:])]\n",
        "    return tf.reshape(x, new_x_shape)\n",
        "\n",
        "def split_heads(x, n, k=False):\n",
        "    if k:\n",
        "        return tf.transpose(split_states(x, n), [0, 2, 3, 1])\n",
        "    else:\n",
        "        return tf.transpose(split_states(x, n), [0, 2, 1, 3])\n",
        "\n",
        "def merge_heads(x):\n",
        "    return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n",
        "\n",
        "def conv1d(x, scope, nf, rf, w_init=tf.random_normal_initializer(stddev=0.02), b_init=tf.constant_initializer(0), pad='VALID', train=False):\n",
        "    with tf.variable_scope(scope):\n",
        "        nx = shape_list(x)[-1]\n",
        "        w = tf.get_variable(\"w\", [rf, nx, nf], initializer=w_init)\n",
        "        b = tf.get_variable(\"b\", [nf], initializer=b_init)\n",
        "        if rf == 1: #faster 1x1 conv\n",
        "            c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, shape_list(x)[:-1]+[nf])\n",
        "        else: #was used to train LM\n",
        "            c = tf.nn.conv1d(x, w, stride=1, padding=pad)+b\n",
        "        return c\n",
        "\n",
        "def attn(x, scope, n_state, n_head, train=False, scale=False):\n",
        "    assert n_state%n_head==0\n",
        "    with tf.variable_scope(scope):\n",
        "        c = conv1d(x, 'c_attn', n_state*3, 1, train=train)\n",
        "        q, k, v = tf.split(c, 3, 2)\n",
        "        q = split_heads(q, n_head)\n",
        "        k = split_heads(k, n_head, k=True)\n",
        "        v = split_heads(v, n_head)\n",
        "        a = _attn(q, k, v, train=train, scale=scale)\n",
        "        a = merge_heads(a)\n",
        "        a = conv1d(a, 'c_proj', n_state, 1, train=train)\n",
        "        a = dropout(a, resid_pdrop, train)\n",
        "        return a\n",
        "\n",
        "def mlp(x, scope, n_state, train=False):\n",
        "    with tf.variable_scope(scope):\n",
        "        nx = shape_list(x)[-1]\n",
        "        act = act_fns[afn]\n",
        "        h = act(conv1d(x, 'c_fc', n_state, 1, train=train))\n",
        "        h2 = conv1d(h, 'c_proj', nx, 1, train=train)\n",
        "        h2 = dropout(h2, resid_pdrop, train)\n",
        "        return h2\n",
        "\n",
        "def block(x, scope, train=False, scale=False):\n",
        "    with tf.variable_scope(scope):\n",
        "        nx = shape_list(x)[-1]\n",
        "        a = attn(x, 'attn', nx, n_head, train=train, scale=scale)\n",
        "        n = norm(x+a, 'ln_1')\n",
        "        m = mlp(n, 'mlp', nx*4, train=train)\n",
        "        h = norm(n+m, 'ln_2')\n",
        "        return h\n",
        "\n",
        "def embed(X, we):\n",
        "    we = convert_gradient_to_tensor(we)\n",
        "    e = tf.gather(we, X)\n",
        "    h = tf.reduce_sum(e, 2)\n",
        "    return h\n",
        "\n",
        "def clf(x, ny, w_init=tf.random_normal_initializer(stddev=0.02), b_init=tf.constant_initializer(0), train=False):\n",
        "    with tf.variable_scope('clf'):\n",
        "        nx = shape_list(x)[-1]\n",
        "        w = tf.get_variable(\"w\", [nx, ny], initializer=w_init)\n",
        "        b = tf.get_variable(\"b\", [ny], initializer=b_init)\n",
        "        return tf.matmul(x, w)+b\n",
        "\n",
        "def model(X, M, Y, train=False, reuse=False):\n",
        "    with tf.variable_scope('model', reuse=reuse):\n",
        "        we = tf.get_variable(\"we\", [n_vocab+n_special+n_ctx, n_embd], initializer=tf.random_normal_initializer(stddev=0.02))\n",
        "        we = dropout(we, embd_pdrop, train)\n",
        "\n",
        "        X = tf.reshape(X, [-1, n_ctx, 2])\n",
        "        M = tf.reshape(M, [-1, n_ctx])\n",
        "\n",
        "        h = embed(X, we)\n",
        "        for layer in range(n_layer):\n",
        "            h = block(h, 'h%d'%layer, train=train, scale=True)\n",
        "\n",
        "        lm_h = tf.reshape(h[:, :-1], [-1, n_embd])\n",
        "        lm_logits = tf.matmul(lm_h, we, transpose_b=True)\n",
        "        lm_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=lm_logits, labels=tf.reshape(X[:, 1:, 0], [-1]))\n",
        "        lm_losses = tf.reshape(lm_losses, [shape_list(X)[0], shape_list(X)[1]-1])\n",
        "        lm_losses = tf.reduce_sum(lm_losses*M[:, 1:], 1)/tf.reduce_sum(M[:, 1:], 1)\n",
        "\n",
        "        clf_h = tf.reshape(h, [-1, n_embd])\n",
        "        pool_idx = tf.cast(tf.argmax(tf.cast(tf.equal(X[:, :, 0], clf_token), tf.float32), 1), tf.int32)\n",
        "        clf_h = tf.gather(clf_h, tf.range(shape_list(X)[0], dtype=tf.int32)*n_ctx+pool_idx)\n",
        "\n",
        "        clf_h = tf.reshape(clf_h, [-1, 2, n_embd])\n",
        "        if train and clf_pdrop > 0:\n",
        "            shape = shape_list(clf_h)\n",
        "            shape[1] = 1\n",
        "            clf_h = tf.nn.dropout(clf_h, 1-clf_pdrop, shape)\n",
        "        clf_h = tf.reshape(clf_h, [-1, n_embd])\n",
        "        clf_logits = clf(clf_h, 1, train=train)\n",
        "        clf_logits = tf.reshape(clf_logits, [-1, 2])\n",
        "\n",
        "        clf_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=clf_logits, labels=Y)\n",
        "        return clf_logits, clf_losses, lm_losses\n",
        "\n",
        "def mgpu_train(*xs):\n",
        "    gpu_ops = []\n",
        "    gpu_grads = []\n",
        "    xs = (tf.split(x, n_gpu, 0) for x in xs)\n",
        "    for i, xs in enumerate(zip(*xs)):\n",
        "        do_reuse = True if i > 0 else None\n",
        "        with tf.device(assign_to_gpu(i, \"/gpu:0\")), tf.variable_scope(tf.get_variable_scope(), reuse=do_reuse):\n",
        "            clf_logits, clf_losses, lm_losses = model(*xs, train=True, reuse=do_reuse)\n",
        "            if lm_coef > 0:\n",
        "                train_loss = tf.reduce_mean(clf_losses) + lm_coef*tf.reduce_mean(lm_losses)\n",
        "            else:\n",
        "                train_loss = tf.reduce_mean(clf_losses)\n",
        "            params = find_trainable_variables(\"model\")\n",
        "            grads = tf.gradients(train_loss, params)\n",
        "            grads = list(zip(grads, params))\n",
        "            gpu_grads.append(grads)\n",
        "            gpu_ops.append([clf_logits, clf_losses, lm_losses])\n",
        "    ops = [tf.concat(op, 0) for op in zip(*gpu_ops)]\n",
        "    grads = average_grads(gpu_grads)\n",
        "    grads = [g for g, p in grads]\n",
        "    train = opt_fns[opt](params, grads, lr, partial(lr_schedules[lr_schedule], warmup=lr_warmup), n_updates_total, l2=l2, max_grad_norm=max_grad_norm, vector_l2=vector_l2, b1=b1, b2=b2, e=e)\n",
        "    return [train]+ops\n",
        "\n",
        "def mgpu_predict(*xs):\n",
        "    gpu_ops = []\n",
        "    xs = (tf.split(x, n_gpu, 0) for x in xs)\n",
        "    for i, xs in enumerate(zip(*xs)):\n",
        "        with tf.device(assign_to_gpu(i, \"/gpu:0\")), tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
        "            clf_logits, clf_losses, lm_losses = model(*xs, train=False, reuse=True)\n",
        "            gpu_ops.append([clf_logits, clf_losses, lm_losses])\n",
        "    ops = [tf.concat(op, 0) for op in zip(*gpu_ops)]\n",
        "    return ops\n",
        "\n",
        "def transform_roc(X1, X2, X3):\n",
        "    n_batch = len(X1)\n",
        "    xmb = np.zeros((n_batch, 2, n_ctx, 2), dtype=np.int32)\n",
        "    mmb = np.zeros((n_batch, 2, n_ctx), dtype=np.float32)\n",
        "    start = encoder['_start_']\n",
        "    delimiter = encoder['_delimiter_']\n",
        "    for i, (x1, x2, x3), in enumerate(zip(X1, X2, X3)):\n",
        "        x12 = [start]+x1[:max_len]+[delimiter]+x2[:max_len]+[clf_token]\n",
        "        x13 = [start]+x1[:max_len]+[delimiter]+x3[:max_len]+[clf_token]\n",
        "        l12 = len(x12)\n",
        "        l13 = len(x13)\n",
        "        xmb[i, 0, :l12, 0] = x12\n",
        "        xmb[i, 1, :l13, 0] = x13\n",
        "        mmb[i, 0, :l12] = 1\n",
        "        mmb[i, 1, :l13] = 1\n",
        "    xmb[:, :, :, 1] = np.arange(n_vocab+n_special, n_vocab+n_special+n_ctx)\n",
        "    return xmb, mmb\n",
        "\n",
        "def iter_apply(Xs, Ms, Ys):\n",
        "    fns = [lambda x:np.concatenate(x, 0), lambda x:float(np.sum(x))]\n",
        "    results = []\n",
        "    for xmb, mmb, ymb in iter_data(Xs, Ms, Ys, n_batch=n_batch_train, truncate=False, verbose=True):\n",
        "        n = len(xmb)\n",
        "        if n == n_batch_train:\n",
        "            res = sess.run([eval_mgpu_logits, eval_mgpu_clf_loss], {X_train:xmb, M_train:mmb, Y_train:ymb})\n",
        "        else:\n",
        "            res = sess.run([eval_logits, eval_clf_loss], {X:xmb, M:mmb, Y:ymb})\n",
        "        res = [r*n for r in res]\n",
        "        results.append(res)\n",
        "    results = zip(*results)\n",
        "    return [fn(res) for res, fn in zip(results, fns)]\n",
        "\n",
        "def iter_predict(Xs, Ms):\n",
        "    logits = []\n",
        "    for xmb, mmb in iter_data(Xs, Ms, n_batch=n_batch_train, truncate=False, verbose=True):\n",
        "        n = len(xmb)\n",
        "        if n == n_batch_train:\n",
        "            logits.append(sess.run(eval_mgpu_logits, {X_train:xmb, M_train:mmb}))\n",
        "        else:\n",
        "            logits.append(sess.run(eval_logits, {X:xmb, M:mmb}))\n",
        "    logits = np.concatenate(logits, 0)\n",
        "    return logits\n",
        "\n",
        "def save(path):\n",
        "    ps = sess.run(params)\n",
        "    joblib.dump(ps, make_path(path))\n",
        "\n",
        "def log():\n",
        "    global best_score\n",
        "    tr_logits, tr_cost = iter_apply(trX[:n_valid], trM[:n_valid], trY[:n_valid])\n",
        "    va_logits, va_cost = iter_apply(vaX, vaM, vaY)\n",
        "    tr_cost = tr_cost/len(trY[:n_valid])\n",
        "    va_cost = va_cost/n_valid\n",
        "    tr_acc = accuracy_score(trY[:n_valid], np.argmax(tr_logits, 1))*100.\n",
        "    va_acc = accuracy_score(vaY, np.argmax(va_logits, 1))*100.\n",
        "    logger.log(n_epochs=n_epochs, n_updates=n_updates, tr_cost=tr_cost, va_cost=va_cost, tr_acc=tr_acc, va_acc=va_acc)\n",
        "    print('%d %d %.3f %.3f %.2f %.2f'%(n_epochs, n_updates, tr_cost, va_cost, tr_acc, va_acc))\n",
        "    if submit:\n",
        "        score = va_acc\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            save(os.path.join(save_dir, desc, 'best_params.jl'))\n",
        "\n",
        "argmax = lambda x:np.argmax(x, 1)\n",
        "\n",
        "pred_fns = {\n",
        "    'rocstories':argmax,\n",
        "}\n",
        "\n",
        "filenames = {\n",
        "    'rocstories':'ROCStories.tsv',\n",
        "}\n",
        "\n",
        "label_decoders = {\n",
        "    'rocstories':None,\n",
        "}\n",
        "\n",
        "def predict():\n",
        "    filename = filenames[dataset]\n",
        "    pred_fn = pred_fns[dataset]\n",
        "    label_decoder = label_decoders[dataset]\n",
        "    predictions = pred_fn(iter_predict(teX, teM))\n",
        "    if label_decoder is not None:\n",
        "        predictions = [label_decoder[prediction] for prediction in predictions]\n",
        "    path = os.path.join(submission_dir, filename)\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, 'w') as f:\n",
        "        f.write('{}\\t{}\\n'.format('index', 'prediction'))\n",
        "        for i, prediction in enumerate(predictions):\n",
        "            f.write('{}\\t{}\\n'.format(i, prediction))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--desc', type=str)\n",
        "    parser.add_argument('--dataset', type=str)\n",
        "    parser.add_argument('--log_dir', type=str, default='log/')\n",
        "    parser.add_argument('--save_dir', type=str, default='save/')\n",
        "    parser.add_argument('--data_dir', type=str, default='data/')\n",
        "    parser.add_argument('--submission_dir', type=str, default='submission/')\n",
        "    parser.add_argument('--submit', action='store_true')\n",
        "    parser.add_argument('--analysis', action='store_true')\n",
        "    parser.add_argument('--seed', type=int, default=42)\n",
        "    parser.add_argument('--n_iter', type=int, default=3)\n",
        "    parser.add_argument('--n_batch', type=int, default=8)\n",
        "    parser.add_argument('--max_grad_norm', type=int, default=1)\n",
        "    parser.add_argument('--lr', type=float, default=6.25e-5)\n",
        "    parser.add_argument('--lr_warmup', type=float, default=0.002)\n",
        "    parser.add_argument('--n_ctx', type=int, default=512)\n",
        "    parser.add_argument('--n_embd', type=int, default=768)\n",
        "    parser.add_argument('--n_head', type=int, default=12)\n",
        "    parser.add_argument('--n_layer', type=int, default=12)\n",
        "    parser.add_argument('--embd_pdrop', type=float, default=0.1)\n",
        "    parser.add_argument('--attn_pdrop', type=float, default=0.1)\n",
        "    parser.add_argument('--resid_pdrop', type=float, default=0.1)\n",
        "    parser.add_argument('--clf_pdrop', type=float, default=0.1)\n",
        "    parser.add_argument('--l2', type=float, default=0.01)\n",
        "    parser.add_argument('--vector_l2', action='store_true')\n",
        "    parser.add_argument('--n_gpu', type=int, default=4)\n",
        "    parser.add_argument('--opt', type=str, default='adam')\n",
        "    parser.add_argument('--afn', type=str, default='gelu')\n",
        "    parser.add_argument('--lr_schedule', type=str, default='warmup_linear')\n",
        "    parser.add_argument('--encoder_path', type=str, default='model/encoder_bpe_40000.json')\n",
        "    parser.add_argument('--bpe_path', type=str, default='model/vocab_40000.bpe')\n",
        "    parser.add_argument('--n_transfer', type=int, default=12)\n",
        "    parser.add_argument('--lm_coef', type=float, default=0.5)\n",
        "    parser.add_argument('--b1', type=float, default=0.9)\n",
        "    parser.add_argument('--b2', type=float, default=0.999)\n",
        "    parser.add_argument('--e', type=float, default=1e-8)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    print(args)\n",
        "    globals().update(args.__dict__)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.set_random_seed(seed)\n",
        "\n",
        "    logger = ResultLogger(path=os.path.join(log_dir, '{}.jsonl'.format(desc)), **args.__dict__)\n",
        "    text_encoder = TextEncoder(encoder_path, bpe_path)\n",
        "    encoder = text_encoder.encoder\n",
        "    n_vocab = len(text_encoder.encoder)\n",
        "\n",
        "    (trX1, trX2, trX3, trY), (vaX1, vaX2, vaX3, vaY), (teX1, teX2, teX3) = encode_dataset(rocstories(data_dir), encoder=text_encoder)\n",
        "    n_y = 2\n",
        "    encoder['_start_'] = len(encoder)\n",
        "    encoder['_delimiter_'] = len(encoder)\n",
        "    encoder['_classify_'] = len(encoder)\n",
        "    clf_token = encoder['_classify_']\n",
        "    n_special = 3\n",
        "    max_len = n_ctx//2-2\n",
        "    n_ctx = min(max([len(x1[:max_len])+max(len(x2[:max_len]), len(x3[:max_len])) for x1, x2, x3 in zip(trX1, trX2, trX3)]+[len(x1[:max_len])+max(len(x2[:max_len]), len(x3[:max_len])) for x1, x2, x3 in zip(vaX1, vaX2, vaX3)]+[len(x1[:max_len])+max(len(x2[:max_len]), len(x3[:max_len])) for x1, x2, x3 in zip(teX1, teX2, teX3)])+3, n_ctx)\n",
        "    trX, trM = transform_roc(trX1, trX2, trX3)\n",
        "    vaX, vaM = transform_roc(vaX1, vaX2, vaX3)\n",
        "    if submit:\n",
        "        teX, teM = transform_roc(teX1, teX2, teX3)\n",
        "\n",
        "    n_train = len(trY)\n",
        "    n_valid = len(vaY)\n",
        "    n_batch_train = n_batch*n_gpu\n",
        "    n_updates_total = (n_train//n_batch_train)*n_iter\n",
        "\n",
        "    X_train = tf.placeholder(tf.int32, [n_batch_train, 2, n_ctx, 2])\n",
        "    M_train = tf.placeholder(tf.float32, [n_batch_train, 2, n_ctx])\n",
        "    X = tf.placeholder(tf.int32, [None, 2, n_ctx, 2])\n",
        "    M = tf.placeholder(tf.float32, [None, 2, n_ctx])\n",
        "\n",
        "    Y_train = tf.placeholder(tf.int32, [n_batch_train])\n",
        "    Y = tf.placeholder(tf.int32, [None])\n",
        "\n",
        "    train, logits, clf_losses, lm_losses = mgpu_train(X_train, M_train, Y_train)\n",
        "    clf_loss = tf.reduce_mean(clf_losses)\n",
        "\n",
        "    params = find_trainable_variables('model')\n",
        "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    shapes = json.load(open('model/params_shapes.json'))\n",
        "    offsets = np.cumsum([np.prod(shape) for shape in shapes])\n",
        "    init_params = [np.load('model/params_{}.npy'.format(n)) for n in range(10)]\n",
        "    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]\n",
        "    init_params = [param.reshape(shape) for param, shape in zip(init_params, shapes)]\n",
        "    init_params[0] = init_params[0][:n_ctx]\n",
        "    init_params[0] = np.concatenate([init_params[1], (np.random.randn(n_special, n_embd)*0.02).astype(np.float32), init_params[0]], 0)\n",
        "    del init_params[1]\n",
        "\n",
        "    if n_transfer == -1:\n",
        "        n_transfer = 0\n",
        "    else:\n",
        "        n_transfer = 1+n_transfer*12\n",
        "    sess.run([p.assign(ip) for p, ip in zip(params[:n_transfer], init_params[:n_transfer])])\n",
        "\n",
        "    eval_mgpu_logits, eval_mgpu_clf_losses, eval_mgpu_lm_losses = mgpu_predict(X_train, M_train, Y_train)\n",
        "    eval_logits, eval_clf_losses, eval_lm_losses = model(X, M, Y, train=False, reuse=True)\n",
        "    eval_clf_loss = tf.reduce_mean(eval_clf_losses)\n",
        "    eval_mgpu_clf_loss = tf.reduce_mean(eval_mgpu_clf_losses)\n",
        "\n",
        "    n_updates = 0\n",
        "    n_epochs = 0\n",
        "    if dataset != 'stsb':\n",
        "        trYt = trY\n",
        "    if submit:\n",
        "        save(os.path.join(save_dir, desc, 'best_params.jl'))\n",
        "    best_score = 0\n",
        "    for i in range(n_iter):\n",
        "        for xmb, mmb, ymb in iter_data(*shuffle(trX, trM, trYt, random_state=np.random), n_batch=n_batch_train, truncate=True, verbose=True):\n",
        "            cost, _ = sess.run([clf_loss, train], {X_train:xmb, M_train:mmb, Y_train:ymb})\n",
        "            n_updates += 1\n",
        "            if n_updates in [1000, 2000, 4000, 8000, 16000, 32000] and n_epochs == 0:\n",
        "                log()\n",
        "        n_epochs += 1\n",
        "        log()\n",
        "    if submit:\n",
        "        sess.run([p.assign(ip) for p, ip in zip(params, joblib.load(os.path.join(save_dir, desc, 'best_params.jl')))])\n",
        "        predict()\n",
        "        if analysis:\n",
        "            rocstories_analysis(data_dir, os.path.join(submission_dir, 'ROCStories.tsv'), os.path.join(log_dir, 'rocstories.jsonl'))"
      ]
    }
  ]
}